{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7920e4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Download\n",
    "     #tqdm\n",
    "     #torchvision\n",
    "     #cv2   \n",
    "#Pytorch dataset \n",
    "from tqdm import tqdm\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from astropy.io import fits\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "#Deep Learning\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision.utils import save_image\n",
    "#images#\n",
    "import cv2\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "#sesion save# \n",
    "import dill\n",
    "\n",
    "#\n",
    "from datasetImages import DatasetImages\n",
    "from datasetDirty import DatasetDirty\n",
    "from datasetPSF import DatasetPSF\n",
    "import cupy as cp\n",
    "from paramsEllipses import ParamsEllipses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab2602c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 28 # image size\n",
    "N_PSF = 28 # psf size\n",
    "TYPE_PSF = 'psf_gauss_'+str(N)+'x'+str(N)\n",
    "# **Params Dataset Python**\n",
    "PATH_IMAGES = '../datasets/images_'+str(N)+'x'+str(N)+'/images' #path dataset images\n",
    "PATH_CONVOLUTION = '../datasets/images_'+str(N)+'x'+str(N)+'/convolutions/'+TYPE_PSF+'/conv'#path dataset convolution\n",
    "PATH_PSF_SAVE =  '../datasets/images_'+str(N)+'x'+str(N)+'/convolutions/'+TYPE_PSF+'/psf'#path convolution\n",
    "INITIAL_DATASET = 0  #initial index for the names of the saved images \n",
    "FINAL_DATASET =   10000#final index for the names of the saved images \n",
    "STEP_DATASET = 1000\n",
    "PERC_TRAIN = 0.7 # (70%) #train percentage \n",
    "PERC_VAL = 0.3 #  (25#) #validation percentage\n",
    "#PERC_TEST =  0.05 # (5%) #test percentag\n",
    "\n",
    "BATCH_TRAIN = 10\n",
    "BATCH_VALIDATION = 10 #batch sizeimages\n",
    "BATCH_TEST = 1\n",
    "\n",
    "NUM_EPOCHS = 10\n",
    "LEARNING_RATE = 1e-3\n",
    "size_data= FINAL_DATASET - INITIAL_DATASET\n",
    "PATH_VALIDATION =  '../datasets/images_'+str(N)+'x'+str(N)+'/convolutions/'+TYPE_PSF+'/models/model_S'+str(size_data)+'_B'+str(BATCH_TRAIN)+'_N'+str(NUM_EPOCHS)+'/validation'\n",
    "PATH_TEST =  '../datasets/images_'+str(N)+'x'+str(N)+'/convolutions/'+TYPE_PSF+'/models/model_S'+str(size_data)+'_B'+str(BATCH_TRAIN)+'_N'+str(NUM_EPOCHS)+'/test'\n",
    "PATH_GRAPH ='../datasets/images_'+str(N)+'x'+str(N)+'/convolutions/'+TYPE_PSF+'/models/model_S'+str(size_data)+'_B'+str(BATCH_TRAIN)+'_N'+str(NUM_EPOCHS)+'/graph'\n",
    "PATH_MODEL_SAVE ='../datasets/images_'+str(N)+'x'+str(N)+'/convolutions/'+TYPE_PSF+'/models/model_S'+str(size_data)+'_B'+str(BATCH_TRAIN)+'_N'+str(NUM_EPOCHS)+'/model' #path where the dataset convolution is saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a28857",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_decoded_image(img, name):\n",
    "    img = img.view(img.size(0), 1, N, N)\n",
    "    save_image(img, name)\n",
    "    \n",
    "def make_dir(path):\n",
    "        if not os.path.exists(path):\n",
    "            os.makedirs(path)\n",
    "\n",
    "def get_device():\n",
    "    if torch.cuda.is_available():\n",
    "        device = 'cuda:0'\n",
    "    else:\n",
    "        device = 'cpu'\n",
    "    return device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f77066",
   "metadata": {},
   "outputs": [],
   "source": [
    "tsfms=transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "\n",
    "class interferometryDataset(Dataset):\n",
    "  def __init__(self,datasetnoised,datasetclean,transform):\n",
    "    self.noise=datasetnoised\n",
    "    self.clean=datasetclean\n",
    "    self.transform=transform\n",
    "  \n",
    "  def __len__(self):\n",
    "    return len(self.noise)\n",
    "  \n",
    "  def __getitem__(self,idx):\n",
    "    xNoise=self.noise[idx]\n",
    "    xClean=self.clean[idx]\n",
    "    if self.transform != None:\n",
    "      xNoise=self.transform(cp.asnumpy(xNoise))\n",
    "      xClean=self.transform(cp.asnumpy(xClean))\n",
    "\n",
    "    return (xNoise,xClean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8304730",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_data_arange(index):\n",
    "\n",
    "    size = STEP_DATASET\n",
    "    size_train = round(size*PERC_TRAIN)\n",
    "    batch_train_size=  BATCH_TRAIN \n",
    "\n",
    "    params = ParamsEllipses(N)\n",
    "\n",
    "    data_image = DatasetImages(N)\n",
    "    data_image.create(size_image=N,params = params , start = index,stop = index +size_train)\n",
    "\n",
    "    data_psf = DatasetPSF(N,TYPE_PSF)\n",
    "    data_psf.create(N,TYPE_PSF)\n",
    "\n",
    "    data_dirty = DatasetDirty(N,TYPE_PSF,data_psf.image)\n",
    "    data_dirty.create(images = data_image.images,size_image = N, type_psf = TYPE_PSF, psf =data_psf.image)\n",
    "    trainSet=interferometryDataset(data_dirty.dirtys,data_image.images,tsfms)\n",
    "    trainLoader=DataLoader(trainSet,batch_train_size,shuffle=True)\n",
    "    return trainLoader\n",
    "\n",
    "\n",
    "def validation_data_arange(index):\n",
    "    size = STEP_DATASET\n",
    "    index = index + round(size*PERC_TRAIN)\n",
    "    size_validation =  round(size*PERC_VAL)\n",
    "    batch_validation_size=  BATCH_VALIDATION\n",
    "\n",
    "    params = ParamsEllipses(N)\n",
    "\n",
    "    data_image = DatasetImages(N)\n",
    "    data_image.create(size_image=N,params = params , start = index,stop = index +size_validation)\n",
    "\n",
    "    data_psf = DatasetPSF(N,TYPE_PSF)\n",
    "    data_psf.create(N,TYPE_PSF)\n",
    "\n",
    "    data_dirty = DatasetDirty(N,TYPE_PSF,data_psf.image)\n",
    "    data_dirty.create(images = data_image.images,size_image = N, type_psf = TYPE_PSF, psf =data_psf.image)\n",
    "    validationSet = interferometryDataset(data_dirty.dirtys,data_image.images,tsfms)\n",
    "    validationLoader=DataLoader(validationSet,batch_validation_size,shuffle=True)\n",
    "    return validationLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334e83c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        # encoder layers\n",
    "        self.enc1 = nn.Conv2d(1, 64, kernel_size=3, padding=1)\n",
    "        self.enc2 = nn.Conv2d(64, 32, kernel_size=3, padding=1)\n",
    "        self.enc3 = nn.Conv2d(32, 16, kernel_size=3, padding=1)\n",
    "        self.enc4 = nn.Conv2d(16, 8, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        # decoder layers\n",
    "        self.dec1 = nn.ConvTranspose2d(8, 8, kernel_size=3, stride=2)  \n",
    "        self.dec2 = nn.ConvTranspose2d(8, 16, kernel_size=3, stride=2)\n",
    "        self.dec3 = nn.ConvTranspose2d(16, 32, kernel_size=2, stride=2)\n",
    "        self.dec4 = nn.ConvTranspose2d(32, 64, kernel_size=2, stride=2)\n",
    "        self.out = nn.Conv2d(64, 1, kernel_size=3, padding=1)\n",
    "    def forward(self, x):\n",
    "        # encode\n",
    "        x = F.relu(self.enc1(x))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.enc2(x))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.enc3(x))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.enc4(x))\n",
    "        x = self.pool(x) # the latent space representation\n",
    "        \n",
    "        # decode\n",
    "        x = F.relu(self.dec1(x))\n",
    "        x = F.relu(self.dec2(x))\n",
    "        x = F.relu(self.dec3(x))\n",
    "        x = F.relu(self.dec4(x))\n",
    "        x = F.sigmoid(self.out(x))\n",
    "        return x\n",
    "net = Autoencoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14279420",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "# the optimizaater\n",
    "optimizer = optim.Adam(net.parameters(), lr=LEARNING_RATE)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af4d7009",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train(net, NUM_EPOCHS,path):\n",
    "  train_loss = []\n",
    "  valid_loss = []\n",
    "  list_index = cp.arange(INITIAL_DATASET,FINAL_DATASET,STEP_DATASET)\n",
    "  for epoch in range(NUM_EPOCHS):\n",
    "    running_loss = 0.0\n",
    "    size_train = 0\n",
    "    for index in list_index:\n",
    "        trainLoader = train_data_arange(index)\n",
    "        size_train = size_train +len(trainLoader)\n",
    "        print('train size:'+str(size_train))\n",
    "        for dirty,clean in tqdm((trainLoader)):\n",
    "          dirty,clean=dirty.to(device).float(),clean.to(device).float()\n",
    "\n",
    "          optimizer.zero_grad()            \n",
    "          outputs = net(dirty)\n",
    "          loss = criterion(outputs, clean)\n",
    "          # backpropagation\n",
    "          loss.backward()\n",
    "          # update the parameters\n",
    "          optimizer.step()\n",
    "          running_loss += loss.item()\n",
    "    loss = running_loss / size_train\n",
    "    train_loss.append(loss)\n",
    "    \n",
    "      ## validation ## '''\n",
    "    validation_loss = 0.0\n",
    "    net.eval()\n",
    "    size_validate = 0  \n",
    "    for index in list_index:\n",
    "        validationloader = validation_data_arange(index)\n",
    "        size_validate = size_validate + len(validationloader)\n",
    "        print('validate size:'+str(size_validate))\n",
    "        for dirty, clean in validationloader:\n",
    "            dirty,clean=dirty.to(device).float(),clean.to(device).float()\n",
    "            optimizer.zero_grad()            \n",
    "            outputs = net(dirty)\n",
    "            loss = criterion(outputs, clean)\n",
    "            validation_loss += loss.item()\n",
    "    loss = validation_loss / size_validate\n",
    "    valid_loss.append(loss)\n",
    "    print('Epoch {} of {}, Train Loss: {:.3f}'.format(\n",
    "        epoch+1, NUM_EPOCHS, loss))\n",
    "    '''save_decoded_image(dirty.cpu().data, name='./'+path+'/noisy{}.png'.format(epoch))\n",
    "    save_decoded_image(outputs.cpu().data, name='./'+path+'/denoised{}.png'.format(epoch))\n",
    "    save_decoded_image(clean.cpu().data, name='./'+path+'/clean{}.png'.format(epoch))'''\n",
    "  return net,train_loss,valid_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f7b53f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "device = get_device()\n",
    "print(device)\n",
    "net.to(device)\n",
    "make_dir(PATH_VALIDATION)\n",
    "make_dir(PATH_GRAPH)\n",
    "\n",
    "net,train_loss,validation_loss = train(net, NUM_EPOCHS,PATH_VALIDATION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a19610",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(train_loss)\n",
    "plt.plot(validation_loss)\n",
    "plt.title('Train Loss & Validation Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.savefig(PATH_GRAPH+'/graph_loss_train_validation.png')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
