{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7920e4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "##\n",
    "import cv2\n",
    "\n",
    "#sesion save# \n",
    "import dill\n",
    "import cupy as cp\n",
    "from datasetInterferometry import DatasetInterferometry\n",
    "from auxiliaryFunctions import AuxiliaryFunctions\n",
    "from graph import Graph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af4d7009",
   "metadata": {},
   "outputs": [],
   "source": [
    "class model:\n",
    "    def __init__(self,size_figure,type_psf,num_epochs,learning_rate,batch_train,criterion,optimizer,start,stop,step = None,path_validation= None,path_test=None,path_graph = None,path_model = None):\n",
    "        self.size_figure = size_figure\n",
    "        self.type_psf = type_psf\n",
    "        self.num_epochs = num_epochs\n",
    "        self.learning_rate = learning_rate\n",
    "        self.batch_train = batch_train\n",
    "        self.start = start\n",
    "        self.stop = stop\n",
    "        self.step = step\n",
    "        self.criterion = criterion\n",
    "        self.optimizer = optimizer\n",
    "        self.size_data = start-stop\n",
    "        self.path_test  =  self.init_path_test(path_test)\n",
    "        self.path_graph =  self.init_path_graph(path_graph)\n",
    "        self.path_model = self.init_path_model(path_model)\n",
    "         \n",
    "    def init_path_test(self,path_test):\n",
    "        if (path_test == None):\n",
    "            return '../datasets/images_'+str(self.size_figure )+'x'+str(self.size_figure )+'/convolutions/'+self.type_psf+'/models/model_S'+str(self.size_data)+'_B'+str(self.batch_train)+'_N'+str(self.num_epochs)+'/test'\n",
    "        else:\n",
    "            return path_test \n",
    "        \n",
    "    def init_path_graph(self,path_graph):\n",
    "        if (path_test == None):\n",
    "            return '../datasets/images_'+str(self.size_figure )+'x'+str(self.size_figure )+'/convolutions/'+self.type_psf+'/models/model_S'+str(self.size_data)+'_B'+str(self.batch_train)+'_N'+str(self.num_epochs)+'/graph'\n",
    "        else:\n",
    "            return path_graph \n",
    "        \n",
    "    def init_path_model(self,path_model):\n",
    "         if (path_model == None):\n",
    "            return '../datasets/images_'+str(self.size_figure )+'x'+str(self.size_figure )+'/convolutions/'+self.type_psf+'/models/model_S'+str(self.size_data)+'_B'+str(self.batch_train)+'_N'+str( self.num_epochs)+'/model' \n",
    "         else:\n",
    "            return path_model \n",
    "  \n",
    "    \n",
    "    def train_data_memory(self,net,start,stop,step):\n",
    "        train_loss = []\n",
    "        valid_loss = []\n",
    "        list_index = cp.arange(start,stop,step)\n",
    "        data = Dataset(self.size_figure,self.type_psf)\n",
    "        for epoch in range(self.num_epochs):\n",
    "            running_loss = 0.0\n",
    "            size_train = 0\n",
    "            for index in list_index:\n",
    "                trainLoader = data.create_train_data(index,step)\n",
    "                size_train = size_train +len(trainLoader)\n",
    "                for dirty,clean in tqdm((trainLoader)):\n",
    "                    dirty,clean=dirty.to(device).float(),clean.to(device).float()\n",
    "                    self.optimizer.zero_grad()            \n",
    "                    outputs = self.net(dirty)\n",
    "                    loss = self.criterion(outputs, clean)\n",
    "                    #backpropagation\n",
    "                    loss.backward()\n",
    "                    #update the parameters\n",
    "                    self.optimizer.step()\n",
    "                    running_loss += loss.item()\n",
    "            loss = running_loss / size_train\n",
    "            train_loss.append(loss)\n",
    "        ## validation ## ''\n",
    "        print('Epoch {} of {}, Train Loss: {:.3f}'.format(epoch+1, self.num_epochs, loss))\n",
    "        validation_loss = 0.0\n",
    "        net.eval()\n",
    "        size_validate = 0  \n",
    "        for index in list_index:\n",
    "            validationloader = data.create_validation_data(index,step)\n",
    "            size_validate = size_validate + len(validationloader)\n",
    "            for dirty, clean in validationloader:\n",
    "                dirty,clean=dirty.to(device).float(),clean.to(device).float()\n",
    "                self.optimizer.zero_grad()    ## TODO BORRRAR ??         \n",
    "                outputs = net(dirty)\n",
    "                loss = criterion(outputs, clean)\n",
    "                validation_loss += loss.item()\n",
    "                loss = validation_loss / size_validate\n",
    "                valid_loss.append(loss)\n",
    "        print('Epoch {} of {}, Validate Loss: {:.3f}'.format(epoch+1, self.num_epochs, loss))\n",
    "        return net,train_loss,valid_loss\n",
    "    \n",
    "    def test_data_memory(self,net,start,stop,step):\n",
    "        list_index = cp.arange(start,stop,step)\n",
    "        data = Dataset(self.size_figure,self.type_psf)\n",
    "        pnsr_1_list = []\n",
    "        pnsr_2_list = []\n",
    "        pnsr_3_list = []\n",
    "        i = 0\n",
    "        for index in list_index:\n",
    "            testloader = data.create_validation_data(index,step)\n",
    "            for dirty,clean in tqdm((testloader)):\n",
    "                dirty=dirty.to(device)\n",
    "                outputs = net(dirty)\n",
    "                dirty  = dirty.cpu().data\n",
    "                outputs = outputs.cpu().data\n",
    "        # psnr #\n",
    "        #psnr_1 = cv2.PSNR(clean.detach().numpy(), outputs.detach().numpy())\n",
    "        #psnr_2 = cv2.PSNR(clean.detach().numpy(),dirty.detach().numpy())\n",
    "        #psnr_3 = cv2.PSNR(dirty.detach().numpy(), outputs.detach().numpy())\n",
    "                psnr_1 = cv2.PSNR(np.array(clean), np.array(outputs))\n",
    "                psnr_2 = cv2.PSNR(np.array(clean), np.array(dirty))\n",
    "                psnr_3 = cv2.PSNR(np.array(dirty), np.array(outputs))\n",
    "                pnsr_1_list.append(psnr_1)\n",
    "                pnsr_2_list.append(psnr_2)\n",
    "                pnsr_3_list.append(psnr_3)\n",
    "                AuxiliaryFunctions.save_decoded_image(dirty, name=self.path_test+'/noisy{}.png'.format(i))\n",
    "                AuxiliaryFunctions.save_decoded_image(outputs, name=self.path_test+'/denoised{}.png'.format(i))\n",
    "                AuxiliaryFunctions.save_decoded_image(clean, name=self.path_test+'/clean{}.png'.format(i))\n",
    "                i = i +1\n",
    "        return net,pnsr_1_list,pnsr_2_list,pnsr_3_list\n",
    "    \n",
    "    \n",
    "      \n",
    "    def train_data(self,net,start,stop):\n",
    "        train_loss = []\n",
    "        valid_loss = []\n",
    "        list_index = cp.arange(start,stop,step)\n",
    "        data = Dataset(self.size_figure,self.type_psf)\n",
    "        for epoch in range(self.num_epochs):\n",
    "            running_loss = 0.0           \n",
    "            trainLoader = data.read_train_data(start,stop):\n",
    "            for dirty,clean in tqdm((trainLoader)):\n",
    "                dirty,clean=dirty.to(device).float(),clean.to(device).float()\n",
    "                self.optimizer.zero_grad()            \n",
    "                outputs = self.net(dirty)\n",
    "                loss = self.criterion(outputs, clean)\n",
    "                #backpropagation\n",
    "                loss.backward()\n",
    "                #update the parameters\n",
    "                self.optimizer.step()\n",
    "                running_loss += loss.item()\n",
    "            loss = running_loss / len(trainLoader)\n",
    "            train_loss.append(loss)\n",
    "        ## validation ## ''\n",
    "        print('Epoch {} of {}, Train Loss: {:.3f}'.format(epoch+1, self.num_epochs, loss))\n",
    "        validation_loss = 0.0\n",
    "        net.eval()\n",
    "        size_validate = 0  \n",
    "        for index in list_index:\n",
    "            validationloader = data.read_validation_data(start,stop)\n",
    "            for dirty, clean in validationloader:\n",
    "                dirty,clean=dirty.to(device).float(),clean.to(device).float()\n",
    "                self.optimizer.zero_grad()    ## TODO BORRRAR ??         \n",
    "                outputs = net(dirty)\n",
    "                loss = criterion(outputs, clean)\n",
    "                validation_loss += loss.item()\n",
    "                loss = validation_loss / len(validationloader)\n",
    "                valid_loss.append(loss)\n",
    "        print('Epoch {} of {}, validate Loss: {:.3f}'.format(epoch+1, self.num_epochs, loss))\n",
    "        return net,train_loss,valid_loss\n",
    "    \n",
    "    def test_data_memory(self,net,start,stop,step):\n",
    "        list_index = cp.arange(start,stop,step)\n",
    "        data = Dataset(self.size_figure,self.type_psf)\n",
    "        pnsr_1_list = []\n",
    "        pnsr_2_list = []\n",
    "        pnsr_3_list = []\n",
    "        i = 0\n",
    "        for index in list_index:\n",
    "            testloader = data.create_validation_data(index,step)\n",
    "            for dirty,clean in tqdm((testloader)):\n",
    "                dirty=dirty.to(device)\n",
    "                outputs = net(dirty)\n",
    "                dirty  = dirty.cpu().data\n",
    "                outputs = outputs.cpu().data\n",
    "        # psnr #\n",
    "        #psnr_1 = cv2.PSNR(clean.detach().numpy(), outputs.detach().numpy())\n",
    "        #psnr_2 = cv2.PSNR(clean.detach().numpy(),dirty.detach().numpy())\n",
    "        #psnr_3 = cv2.PSNR(dirty.detach().numpy(), outputs.detach().numpy())\n",
    "                psnr_1 = cv2.PSNR(np.array(clean), np.array(outputs))\n",
    "                psnr_2 = cv2.PSNR(np.array(clean), np.array(dirty))\n",
    "                psnr_3 = cv2.PSNR(np.array(dirty), np.array(outputs))\n",
    "                pnsr_1_list.append(psnr_1)\n",
    "                pnsr_2_list.append(psnr_2)\n",
    "                pnsr_3_list.append(psnr_3)\n",
    "                AuxiliaryFunctions.save_decoded_image(dirty, name=self.path_test+'/noisy{}.png'.format(i))\n",
    "                AuxiliaryFunctions.save_decoded_image(outputs, name=self.path_test+'/denoised{}.png'.format(i))\n",
    "                AuxiliaryFunctions.save_decoded_image(clean, name=self.path_test+'/clean{}.png'.format(i))\n",
    "                i = i +1\n",
    "        return net,pnsr_1_list,pnsr_2_list,pnsr_3_list\n",
    "    \n",
    "    \n",
    "    \n",
    "    def run_data_memory(self,net,start,stop,step):\n",
    "        torch.cuda.empty_cache()\n",
    "        device = AuxiliaryFunctions.get_device() #### \n",
    "        print(device)\n",
    "        net.to(device)\n",
    "        AuxiliaryFunctions.make_dir(self.path_test)\n",
    "        net,train_loss,validation_loss = self.train_data_memory(net,start,stop,step)\n",
    "        Graph.train_loss_validation_loss_epoch(self.path_graphs,train_loss,validation_loss)        \n",
    "        net,pnsr_1_list,pnsr_2_list,pnsr_3_list = self.test_data_memory(self,self.net,start,stop,step):\n",
    "        Graph.psnr_test(self.path_graphs,pnsr_1_list,pnsr_2_list,pnsr_3_list)\n",
    "\n",
    "        \n",
    "        \n",
    "    def run_data(self,net,start,stop):\n",
    "        torch.cuda.empty_cache()\n",
    "        device = AuxiliaryFunctions.get_device() #### \n",
    "        print(device)\n",
    "        net.to(device)\n",
    "        AuxiliaryFunctions.make_dir(self.path_test)\n",
    "        net,train_loss,validation_loss = self.train_data(net,start,stop)\n",
    "        Graph.train_loss_validation_loss_epoch(self.path_graphs,train_loss,validation_loss)        \n",
    "        net,pnsr_1_list,pnsr_2_list,pnsr_3_list = self.test_data(self,self.net,start,stop):\n",
    "        Graph.psnr_test(self.path_graphs,pnsr_1_list,pnsr_2_list,pnsr_3_list)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
